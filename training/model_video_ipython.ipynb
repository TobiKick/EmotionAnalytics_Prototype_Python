{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# IMPORT STATEMENTS ########################################################\n",
    "#Import Python modules\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "from numpy import asarray\n",
    "from PIL import Image\n",
    "from matplotlib.patches import Rectangle\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from keras_vggface.utils import preprocess_input\n",
    "from keras_vggface.vggface import VGGFace\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "#For local CPU usage:\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "#Import Keras modules\n",
    "from keras.layers import Dense, Flatten, Input, Dropout, Conv1D, Conv2D, LSTM, Concatenate, Reshape, MaxPool1D, MaxPool2D, BatchNormalization\n",
    "from keras import Model, Sequential\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.utils import np_utils\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# SETUP PROJECT PARAMETERS ########################################################\n",
    "LOAD_PROGRESS_FROM_MODEL = False\n",
    "SAVE_PROGRESS_TO_MODEL = True\n",
    "\n",
    "IMAGE_HEIGHT = 224\n",
    "IMAGE_WIDTH = 224\n",
    "\n",
    "PATH_TO_FOLDER = 'C:/Users/Tobias/Desktop/Master-Thesis/Data/AFEW/002'\n",
    "PATH_TO_DATA = 'C:/Users/Tobias/Desktop/Master-Thesis/Data/AFEW'\n",
    "FOLDER = \"/002\"\n",
    "DATA_DIR_PREDICT = 'C:/Users/Tobias/Desktop/Master-Thesis/Data/facesdb/s005/tif/training/'\n",
    "DISPLAY_IMG_NAME = '00000.png'\n",
    "IMG_FORMAT = '.png'\n",
    "\n",
    "batch_size = 32\n",
    "num_folds = 3   # needs to be at least 2 for train and test\n",
    "\n",
    "CROSS_VALIDATION = False\n",
    "EPOCHS = 20\n",
    "EPOCHS_CROSS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach:\n",
    "\n",
    "### 1. Face detection\n",
    "using MTCNN (Simultaneous face detection, face alignment, bounding boxing and landmark detection)\n",
    "\n",
    "### 2. Highlighting faces\n",
    "draw the bounding box in an image and plot it - to check out the result\n",
    "\n",
    "### 3. Face extraction\n",
    "extracting the face according to the identified bounding box\n",
    "\n",
    "### 4. Face recognition\n",
    "Using the VGGFace pretrained Resnet50 model to recognize emotions (training + prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for now reading the RGB values of the image (maybe in future just gray-scale)\n",
    "def get_image_from_filename(path_to_folder, filename):\n",
    "    image = cv2.cvtColor(cv2.imread(os.path.join(path_to_folder, filename)), cv2.COLOR_BGR2RGB)\n",
    "    return image\n",
    "\n",
    "def get_all_images(path_to_data):\n",
    "    images = []\n",
    "    for folder in os.listdir(path_to_data):\n",
    "        for filename in os.listdir(os.path.join(path_to_data, folder)):\n",
    "            img = cv2.imread(os.path.join(path_to_data, folder, filename))\n",
    "            if img is not None:\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                images.append(img)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sructure data according to emotions (necessary to read in labels)\n",
    "def get_labels_from_folder(path_to_data):\n",
    "    labels = []\n",
    "    \n",
    "    for folder in os.listdir(path_to_data):\n",
    "        with open(os.path.join(path_to_data, folder, folder + \".json\")) as p:\n",
    "            data = json.load(p)\n",
    "        frames = data['frames']\n",
    "    \n",
    "        for key, value in frames.items():\n",
    "            labels.append([value['valence'], value['arousal']])\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training, cv and test\n",
    "def get_splitted_data(path_to_data):\n",
    "    labels = get_labels_from_folder(path_to_data)\n",
    "    filenames = get_all_images(path_to_data)\n",
    "    \n",
    "    filenames_shuffled_np, labels_shuffled_np = shuffle(filenames, labels)\n",
    "    X_training, X_test, Y_training, Y_test = train_test_split(filenames_shuffled_np, labels_shuffled_np, test_size=0.2, random_state=1)\n",
    "    \n",
    "    return X_training, X_test, Y_training, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "## 1. Face detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\tobias\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "detector = MTCNN()\n",
    "\n",
    "def detect_face(image):\n",
    "    face = detector.detect_faces(image)\n",
    "    return face\n",
    "\n",
    "def detect_faces(images):\n",
    "    faces = []  \n",
    "    for img in images:\n",
    "        face = detector.detect_faces(img)\n",
    "        if len(face) == 1:\n",
    "            faces.append(face)  ## just use the face with the highest detection probability\n",
    "        elif len(face) > 1:\n",
    "            faces.append(face[0])\n",
    "        else:\n",
    "            faces.append([]) ### no face was detected\n",
    "    return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.1.2) C:\\projects\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-b3dceceece57>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_image_from_filename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPATH_TO_FOLDER\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDISPLAY_IMG_NAME\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mface\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdetect_face\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mface\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mface\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-f8c15d2af199>\u001b[0m in \u001b[0;36mget_image_from_filename\u001b[1;34m(path_to_folder, filename)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# for now reading the RGB values of the image (maybe in future just gray-scale)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_image_from_filename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_to_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_to_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.1.2) C:\\projects\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n"
     ]
    }
   ],
   "source": [
    "image = get_image_from_filename(PATH_TO_FOLDER, DISPLAY_IMG_NAME)\n",
    "face = detect_face(image)\n",
    "print(face)\n",
    "len(face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Higlighting face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_face(path_to_folder, filename):\n",
    "  # display image\n",
    "    image = get_image_from_filename(path_to_folder, filename)\n",
    "    plt.imshow(image)\n",
    "\n",
    "    ax = plt.gca()\n",
    "    face = detect_face(image)\n",
    "    \n",
    "    # for each face in the image, draw a rectangle based on coordinates\n",
    "    for elem in face:\n",
    "        x, y, width, height = elem['box']\n",
    "        face_border = Rectangle((x, y), width, height,\n",
    "                          fill=False, color='red')\n",
    "        ax.add_patch(face_border)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highlight_face(PATH_TO_FOLDER, DISPLAY_IMG_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Face extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_face_from_image(image, required_size=(IMAGE_HEIGHT, IMAGE_WIDTH)):\n",
    "    face = detect_face(image) # content of face is a python dict\n",
    "\n",
    "    # extract the bounding box from the requested face\n",
    "    box = np.asarray(face[0]['box'])\n",
    "    box[box < 0] = 0\n",
    "    x1, y1, width, height =  box\n",
    "    \n",
    "    x2, y2 = x1 + width, y1 + height\n",
    "    # extract the face\n",
    "    face_boundary = image[y1:y2, x1:x2]\n",
    "\n",
    "    # resize pixels to the model size\n",
    "    face_image = Image.fromarray(face_boundary)\n",
    "    face_image = face_image.resize(required_size)\n",
    "    face_array = asarray(face_image)\n",
    "        \n",
    "    return face_array\n",
    "\n",
    "\n",
    "def extract_face_from_images(images, required_size=(IMAGE_HEIGHT, IMAGE_WIDTH)):\n",
    "    faces = detect_faces(images)\n",
    "    face_images = []\n",
    "    \n",
    "    for i in range(len(images)):\n",
    "        if faces[i-1] == []: # No face detected\n",
    "            face_image = Image.fromarray(images[i-1])\n",
    "            face_image = face_image.resize(required_size)\n",
    "            face_array = asarray(face_image)\n",
    "            face_images.append(face_array)\n",
    "        else:\n",
    "            # extract the bounding box from the requested face\n",
    "            if type(faces[i-1]) is list:  # checks whether more than one face was detected\n",
    "                box = np.asarray(faces[i-1][0]['box'])\n",
    "                box[box < 0] = 0\n",
    "                x1, y1, width, height = box\n",
    "            else:\n",
    "                box = np.asarray(faces[i-1]['box'])\n",
    "                box[box < 0] = 0\n",
    "                x1, y1, width, height = box\n",
    "\n",
    "            x2, y2 = x1 + width, y1 + height\n",
    "            # extract the face\n",
    "            face_boundary = images[i-1][y1:y2, x1:x2]\n",
    "\n",
    "            # resize pixels to the model size\n",
    "            face_image = Image.fromarray(face_boundary)\n",
    "            face_image = face_image.resize(required_size)\n",
    "            face_array = asarray(face_image)\n",
    "            face_images.append(face_array)\n",
    "            \n",
    "    return face_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.1.2) C:\\projects\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-aebc332eba80>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Display the first face from the extracted faces\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_image_from_filename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPATH_TO_FOLDER\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDISPLAY_IMG_NAME\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mextracted_face\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_face_from_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextracted_face\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-f8c15d2af199>\u001b[0m in \u001b[0;36mget_image_from_filename\u001b[1;34m(path_to_folder, filename)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# for now reading the RGB values of the image (maybe in future just gray-scale)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_image_from_filename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_to_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_to_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.1.2) C:\\projects\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n"
     ]
    }
   ],
   "source": [
    "# Display the first face from the extracted faces\n",
    "image = get_image_from_filename(PATH_TO_FOLDER, DISPLAY_IMG_NAME)\n",
    "extracted_face = extract_face_from_image(image)\n",
    "plt.imshow(extracted_face)\n",
    "plt.show()\n",
    "extracted_face.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Face recognition -> Emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_face_embedding(faces):\n",
    "    samples = asarray(faces, 'float32')\n",
    "\n",
    "    # prepare the data for the model\n",
    "    samples = preprocess_input(samples, version=2)\n",
    "\n",
    "    # create a vggface model object\n",
    "    model = VGGFace(model='resnet50',\n",
    "        include_top=False,\n",
    "        input_shape=(224, 224, 3),\n",
    "        pooling='avg')\n",
    "    \n",
    "    # perform prediction\n",
    "    return model.predict(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_training, X_test, Y_train, Y_test = get_splitted_data(PATH_TO_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n",
      "20\n",
      "80\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "print(len(X_training))\n",
    "print(len(X_test))\n",
    "print(len(Y_train))\n",
    "print(len(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\tobias\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4074: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_faces = extract_face_from_images(X_training)\n",
    "X_test_faces = extract_face_from_images(X_test)\n",
    "\n",
    "X_train_embeddings = get_face_embedding(X_train_faces)\n",
    "X_test_embeddings = get_face_embedding(X_test_faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('X_train_embeddings.txt', X_train_embeddings)\n",
    "np.savetxt('X_test_embeddings.txt', X_test_embeddings)\n",
    "\n",
    "np.savetxt('Y_train.txt', Y_train)\n",
    "np.savetxt('Y_test.txt', Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_embeddings = np.loadtxt('X_train_embeddings.txt')\n",
    "X_test_embeddings = np.loadtxt('X_test_embeddings.txt')\n",
    "\n",
    "Y_train = np.loadtxt('Y_train.txt')\n",
    "Y_test = np.loadtxt('Y_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the model returns a vector, which represents the features of a face\n",
    "print(X_train_embeddings)\n",
    "print(X_train_embeddings.shape)\n",
    "Y_train = np.asarray(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr(y_true, y_pred):\n",
    "    #normalise\n",
    "    n_y_true = (y_true - K.mean(y_true[:])) / K.std(y_true[:])\n",
    "    n_y_pred = (y_pred - K.mean(y_pred[:])) / K.std(y_pred[:])  \n",
    "\n",
    "    top=K.sum((n_y_true[:]-K.mean(n_y_true[:]))*(n_y_pred[:]-K.mean(n_y_pred[:])),axis=[-1,-2])\n",
    "    bottom=K.sqrt(K.sum(K.pow((n_y_true[:]-K.mean(n_y_true[:])),2),axis=[-1,-2])*K.sum(K.pow(n_y_pred[:]-K.mean(n_y_pred[:]),2),axis=[-1,-2]))\n",
    "\n",
    "    result=top/bottom\n",
    "    return K.mean(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_loss(y_true, y_pred):\n",
    "    x = y_true\n",
    "    y = y_pred\n",
    "    mx = K.mean(x)\n",
    "    my = K.mean(y)\n",
    "    xm, ym = x-mx, y-my\n",
    "    r_num = K.sum(tf.multiply(xm,ym))\n",
    "    r_den = K.sqrt(tf.multiply(K.sum(K.square(xm)), K.sum(K.square(ym))))\n",
    "    r = r_num / r_den\n",
    "\n",
    "    r = K.maximum(K.minimum(r, 1.0), -1.0)\n",
    "    return 1 - K.square(r)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_top(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, activation='relu', input_dim = input_shape))\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(output_dim = 2, activation='tanh')) \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "inputs = np.concatenate((X_train_embeddings, X_test_embeddings), axis=0)\n",
    "targets = np.append(Y_train, Y_test, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_best = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n",
    "mc_es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10) # waiting for 10 consecutive epochs that don't reduce the val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-fold Cross Validation model evaluation\n",
    "history_accuracy = []\n",
    "history_val_accuracy = []\n",
    "history_loss = []\n",
    "history_val_loss = []\n",
    "history_corr = []\n",
    "history_val_corr = []\n",
    "history_rmse = []\n",
    "history_val_rmse = []\n",
    "\n",
    "\n",
    "if CROSS_VALIDATION == True:\n",
    "    for train, test in kfold.split(inputs, targets):\n",
    "        model = model_top(2048)\n",
    "\n",
    "        if LOAD_PROGRESS_FROM_MODEL:\n",
    "            model.load_weights(\"model_checkpoints/model_top.h5\")\n",
    "            print(\"Loaded model from disk\")\n",
    "\n",
    "        model.summary()\n",
    "        model.compile(loss = corr_loss, optimizer = \"adam\", metrics = [\"accuracy\", rmse, corr])\n",
    "\n",
    "        # train model  \n",
    "        scores = model.fit(inputs[train], targets[train], steps_per_epoch = int(len(train) // batch_size), \n",
    "                           epochs=EPOCHS_CROSS, verbose=1, validation_data = (inputs[test], targets[test]),\n",
    "                           validation_steps = int(len(test) // batch_size), callbacks = [mc_best, mc_es])\n",
    "\n",
    "        history_accuracy.extend(scores.history['accuracy'])\n",
    "        history_val_accuracy.extend(scores.history['val_accuracy'])\n",
    "        history_loss.extend(scores.history['loss'])\n",
    "        history_val_loss.extend(scores.history['val_loss'])\n",
    "        history_corr.extend(scores.history['corr'])\n",
    "        history_val_corr.extend(scores.history['val_corr'])\n",
    "        history_rmse.extend(scores.history['rmse'])\n",
    "        history_val_rmse.extend(scores.history['val_rmse'])\n",
    "else:\n",
    "    model = model_top(2048)\n",
    "\n",
    "    if LOAD_PROGRESS_FROM_MODEL:\n",
    "        model.load_weights(\"model_checkpoints/model_top.h5\")\n",
    "        print(\"Loaded model from disk\")\n",
    "\n",
    "    model.summary()\n",
    "    opt = SGD(lr=0.01)  # optimizer = \"adam\"\n",
    "    model.compile(loss = corr_loss, optimizer = opt, metrics = [\"accuracy\", rmse, corr])\n",
    "\n",
    "    # train model  \n",
    "    scores = model.fit(X_train_embeddings, Y_train, epochs=EPOCHS, verbose=1, \n",
    "                       validation_data = (X_test_embeddings, Y_test), callbacks = [mc_best, mc_es])\n",
    "        # steps_per_epoch = int(len(X_train_embeddings) // batch_size)\n",
    "        # validation_steps = int(len(X_test_embeddings) // batch_size)\n",
    "\n",
    "    history_accuracy.extend(scores.history['accuracy'])\n",
    "    history_val_accuracy.extend(scores.history['val_accuracy'])\n",
    "    history_loss.extend(scores.history['loss'])\n",
    "    history_val_loss.extend(scores.history['val_loss'])\n",
    "    history_corr.extend(scores.history['corr'])\n",
    "    history_val_corr.extend(scores.history['val_corr'])\n",
    "    history_rmse.extend(scores.history['rmse'])\n",
    "    history_val_rmse.extend(scores.history['val_rmse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_PROGRESS_TO_MODEL:\n",
    "    model.save_weights(\"model_checkpoints/model_top.h5\")\n",
    "    print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.figure(1)\n",
    "plt.plot(history_accuracy)\n",
    "plt.plot(history_val_accuracy)\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.savefig('model_checkpoints/accuracy.png')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for CORR\n",
    "plt.figure(2)\n",
    "plt.plot(history_corr)\n",
    "plt.plot(history_val_corr)\n",
    "plt.title('model correlation(CORR)')\n",
    "plt.ylabel('correlation')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.savefig('model_checkpoints/correlation.png')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.figure(3)\n",
    "plt.plot(history_loss)\n",
    "plt.plot(history_val_loss)\n",
    "plt.title('model correlation loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.savefig('model_checkpoints/correlation_loss.png')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for RMSE\n",
    "plt.figure(4)\n",
    "plt.plot(history_rmse)\n",
    "plt.plot(history_val_rmse)\n",
    "plt.title('model root_mean_squared_error')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.savefig('model_checkpoints/rmse.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_scores = model.predict(X_test_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99998695 0.99983275]\n",
      " [0.9999988  0.9999997 ]\n",
      " [0.99997723 0.99977726]\n",
      " [0.99999726 0.9999995 ]\n",
      " [0.99999803 0.9999695 ]\n",
      " [0.99999785 0.9999996 ]\n",
      " [0.999983   0.9998684 ]\n",
      " [0.9999985  0.9999771 ]\n",
      " [0.999995   0.99993503]\n",
      " [0.9999882  0.99996454]\n",
      " [0.9999948  0.99986416]\n",
      " [0.99999696 0.99999934]\n",
      " [0.9999894  0.99999905]\n",
      " [0.99999064 0.9998894 ]\n",
      " [0.99999785 0.99999964]\n",
      " [0.99999636 0.99984926]\n",
      " [0.9999278  0.9996679 ]\n",
      " [0.9999962  0.9998706 ]\n",
      " [0.99999744 0.9999994 ]\n",
      " [0.99998343 0.9999043 ]]\n"
     ]
    }
   ],
   "source": [
    "# the model returns the emtions in 2-dimensional Valance-Arousal space\n",
    "print(emotion_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
