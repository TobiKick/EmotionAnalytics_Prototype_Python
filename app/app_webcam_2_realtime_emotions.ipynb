{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import  cv2\n",
    "import numpy as np\n",
    "from keras.models import load_model, Sequential\n",
    "from keras.layers import Dense, BatchNormalization, Dropout\n",
    "\n",
    "from keras_vggface.utils import preprocess_input\n",
    "from keras_vggface.vggface import VGGFace\n",
    "\n",
    "#For local CPU usage:\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "#face detection and extraction\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "from PIL import Image\n",
    "from numpy import asarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_HEIGHT = 224\n",
    "IMAGE_WIDTH = 224\n",
    "\n",
    "EMOTION_DICT = {1:\"ANGRY\", 2:\"DISGUST\", 3:\"FEAR\", 4:\"HAPPY\", 5:\"NEUTRAL\", 6:\"SAD\", 7:\"SURPRISE\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions and model from training, necessary for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\tobias\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From c:\\users\\tobias\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "detector = MTCNN()\n",
    "\n",
    "def detect_faces(image):\n",
    "    faces = detector.detect_faces(image)\n",
    "    return np.array(faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_face_from_image(image, required_size=(IMAGE_HEIGHT, IMAGE_WIDTH)):\n",
    "    face = detect_faces(image) # content of face is a python dict\n",
    "\n",
    "    \n",
    "    print(len(face))\n",
    "    if len(face) == 0:\n",
    "        return []\n",
    "    else:\n",
    "        # extract the bounding box from the requested face\n",
    "        box = np.asarray(face[0]['box'])\n",
    "        box[box < 0] = 0\n",
    "        x1, y1, width, height =  box\n",
    "\n",
    "        x2, y2 = x1 + width, y1 + height\n",
    "        # extract the face\n",
    "        face_boundary = image[y1:y2, x1:x2]\n",
    "\n",
    "        # resize pixels to the model size\n",
    "        face_image = Image.fromarray(face_boundary)\n",
    "        face_image = face_image.resize(required_size)\n",
    "        face_array = asarray(face_image)\n",
    "\n",
    "        return face_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bounding_box(image, required_size=(IMAGE_HEIGHT, IMAGE_WIDTH)):\n",
    "    face = detect_faces(image) # content of face is a python dict\n",
    "    \n",
    "    if len(face) == 0:\n",
    "        return []\n",
    "    elif len(face) == 1:\n",
    "        # extract the bounding box from the requested face\n",
    "        box = np.asarray(face[0]['box'])\n",
    "        box[box < 0] = 0\n",
    "        return np.array(box)\n",
    "    else:\n",
    "        box = []\n",
    "        for elem in face:\n",
    "            b = np.asarray(elem['box'])\n",
    "            b[b < 0] = 0\n",
    "            box.append(b)\n",
    "        return np.array(box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\tobias\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4074: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_VGGFace = VGGFace(model='resnet50', include_top=False, input_shape=(224, 224, 3), pooling='avg')    \n",
    "    \n",
    "def get_face_embedding(face):\n",
    "    samples = asarray(face, 'float32')\n",
    "    \n",
    "    # prepare the data for the model\n",
    "    samples = preprocess_input(samples, version=2)\n",
    "        \n",
    "    return model_VGGFace.predict(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_top(input_shape):\n",
    "    model = Sequential()\n",
    "        \n",
    "    model.add(Dense(512, activation='relu', input_dim = input_shape))\n",
    "    model.add(Dropout(0.1))\n",
    "    \n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    \n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(output_dim = 2, activation='tanh')) \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\tobias\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"tanh\", units=2)`\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "model_top = model_top(2048)\n",
    "model_top.load_weights(\"model_top.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_prediction(path):\n",
    "    #converting image to RGB color and save it\n",
    "    img = cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    #detect face in image, crop it then resize it\n",
    "    face = extract_face_from_image(img)\n",
    "    \n",
    "    if face.ndim == 3:\n",
    "        face = face.reshape((1, face.shape[0], face.shape[1], face.shape[2]))\n",
    "    \n",
    "    #get embedding for face\n",
    "    VGG_pred = get_face_embedding(face)\n",
    "    \n",
    "    #make prediction and display the result\n",
    "    top_pred = model_top.predict(VGG_pred)\n",
    "    \n",
    "    val = int(top_pred[0][0] * 10)\n",
    "    ar = int(top_pred[0][1] * 10)\n",
    "    print(\"result: \" + str(val) + \", \" + str(ar))\n",
    "    return val, ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ## face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml') \n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "def run_app(text, cap):\n",
    "    while(True):\n",
    "        ret, img = cap.read()\n",
    "        img_color = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        cv2.putText(img, str(text), (95,30), font, 0.7, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(img, \"Press SPACE: FOR EMOTION\", (5,470), font, 0.7, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(img, \"Hold Q: To Quit\", (460,470), font, 0.7, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "        \n",
    "        \n",
    "        box = get_bounding_box(img_color)\n",
    "        if box == []:\n",
    "            print(\"No bounding box\")\n",
    "        elif box.ndim == 1:\n",
    "            x,y,w,h = box\n",
    "            cv2.rectangle(img, (x,y), (x+w, y+h), (255, 0, 0), 2)\n",
    "            cv2.imshow(\"Emotion Recognition - MasterThesis\", img)\n",
    "        elif box.ndim >= 2:\n",
    "            for elem in box:\n",
    "                x, y, w, h = elem\n",
    "                cv2.rectangle(img, (x,y), (x+w, y+h), (255, 0, 0), 2)\n",
    "            cv2.imshow(\"Emotion Recognition - MasterThesis\", img)\n",
    "        \n",
    "        if cv2.waitKey(1) == ord(' '):\n",
    "            cv2.imwrite(\"test.jpg\", img)\n",
    "            valence, arousal = return_prediction(\"test.jpg\")\n",
    "            run_app(\"Valence: \" + str(int(valence)) + \", Arousal: \" + str(int(arousal)) , cap)\n",
    "            # run_app(\"No Emotion\", cap)\n",
    "            break\n",
    "            \n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\tobias\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\tobias\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:17: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "result: -9, 10\n",
      "No bounding box\n",
      "No bounding box\n",
      "1\n",
      "result: -9, 10\n",
      "No bounding box\n",
      "No bounding box\n",
      "No bounding box\n",
      "No bounding box\n",
      "No bounding box\n",
      "No bounding box\n",
      "No bounding box\n",
      "No bounding box\n",
      "No bounding box\n",
      "No bounding box\n",
      "No bounding box\n",
      "No bounding box\n",
      "No bounding box\n",
      "No bounding box\n",
      "No bounding box\n",
      "No bounding box\n",
      "No bounding box\n",
      "No bounding box\n",
      "No bounding box\n",
      "No bounding box\n",
      "No bounding box\n",
      "No bounding box\n",
      "No bounding box\n",
      "No bounding box\n",
      "No bounding box\n",
      "No bounding box\n",
      "No bounding box\n",
      "No bounding box\n",
      "No bounding box\n",
      "No bounding box\n",
      "No bounding box\n"
     ]
    }
   ],
   "source": [
    "run_app(\"None\", cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
