{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "c:\\users\\tobias\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\tobias\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\tobias\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\tobias\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\tobias\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\tobias\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "c:\\users\\tobias\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\tobias\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\tobias\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\tobias\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\tobias\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\tobias\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "# import statements\n",
    "import sys\n",
    "from PyQt5.QtCore import *\n",
    "from PyQt5.QtGui import *\n",
    "from PyQt5.QtWidgets import *\n",
    "from PyQt5 import QtWidgets, uic, QtCore, QtGui\n",
    "from PyQt5.QtCore import QObject, pyqtSignal\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pygetwindow as gw\n",
    "\n",
    "# for reading from an window\n",
    "import cv2\n",
    "from PIL import ImageGrab, Image\n",
    "from numpy import asarray\n",
    "import win32api\n",
    "import winGuiAuto\n",
    "import win32gui\n",
    "import win32con\n",
    "import numpy as np\n",
    "import pywintypes\n",
    "\n",
    "#For local CPU usage:\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "from keras_vggface.utils import preprocess_input\n",
    "from keras_vggface.vggface import VGGFace\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "from keras.models import load_model, Sequential, Model\n",
    "from keras.layers import Dense, BatchNormalization, Dropout, Flatten\n",
    "from keras.backend import clear_session, set_session\n",
    "import tensorflow as tf\n",
    "\n",
    "# VARIABLES\n",
    "IPYTHON = True\n",
    "IMAGE_HEIGHT = 224\n",
    "IMAGE_WIDTH = 224\n",
    "\n",
    "INTERVAL_SECONDS = 3  ## Interval seconds between each time an emotion recognition output is produced\n",
    "IMAGES_PER_INTERVAL = 6\n",
    "\n",
    "LAYERS_TRAINABLE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "app_prototype - Jupyter Notebook - Google Chrome\n",
      "Fotocamera\n",
      "React-Portfolio – Header.js IntelliJ IDEA\n",
      "Bon Jovi - I'll Be There For You\n",
      "app_prototype - Jupyter Notebook - Google Chrome\n",
      "['app_prototype - Jupyter Notebook - Google Chrome', 'Fotocamera', 'React-Portfolio – Header.js IntelliJ IDEA', \"Bon Jovi - I'll Be There For You\", 'app_prototype - Jupyter Notebook - Google Chrome']\n"
     ]
    }
   ],
   "source": [
    "def getWindowNames():\n",
    "    visibleWindows = []   \n",
    "    \n",
    "    #if gw.getWindowsWithTitle('Fotocamera'):\n",
    "    #    visibleWindows.append('Fotocamera')\n",
    "    \n",
    "    for i in gw.getAllTitles():        \n",
    "        notepadWindow = gw.getWindowsWithTitle(i)[0]\n",
    "        \n",
    "        if notepadWindow.isMinimized or notepadWindow.isMaximized:\n",
    "            print(notepadWindow.title)\n",
    "            visibleWindows.append(notepadWindow.title)\n",
    "    \n",
    "    return visibleWindows\n",
    "\n",
    "\n",
    "print(getWindowNames())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\tobias\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "input_1\n",
      "conv1_1\n",
      "conv1_2\n",
      "pool1\n",
      "conv2_1\n",
      "conv2_2\n",
      "pool2\n",
      "conv3_1\n",
      "conv3_2\n",
      "conv3_3\n",
      "pool3\n",
      "conv4_1\n",
      "conv4_2\n",
      "conv4_3\n",
      "pool4\n",
      "conv5_1\n",
      "conv5_2\n",
      "conv5_3\n",
      "pool5\n"
     ]
    }
   ],
   "source": [
    "def custom_vgg_model():\n",
    "    vgg_model = VGGFace(include_top=False, input_shape=(224, 224, 3))\n",
    "    \n",
    "    for layer in vgg_model.layers: \n",
    "        layer.trainable = LAYERS_TRAINABLE\n",
    "        print(layer.name)\n",
    "    \n",
    "    last_layer = vgg_model.get_layer('pool5').output    \n",
    "    x = Flatten(name='flatten')(last_layer)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    out1 = Dense(1, activation='tanh', name='out1')(x)\n",
    "    out2 = Dense(1, activation='tanh', name='out2')(x)\n",
    "    custom_vgg_model = Model(inputs= vgg_model.input, outputs= [out1, out2])\n",
    "    \n",
    "    return custom_vgg_model\n",
    "\n",
    "model_top = custom_vgg_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## setting Keras sessions for each of the pretrained networks\n",
    "sess = tf.Session()\n",
    "graph = tf.get_default_graph()\n",
    "set_session(sess)\n",
    "detector = MTCNN()\n",
    "\n",
    "## Second Network\n",
    "# sess2 = tf.Session()\n",
    "# graph = tf.get_default_graph()\n",
    "# set_session(sess2)\n",
    "# model_VGGFace = VGGFace(model='resnet50', include_top=False, input_shape=(224, 224, 3), pooling='avg')\n",
    "\n",
    "# Third Network\n",
    "sess3 = tf.Session()\n",
    "graph = tf.get_default_graph()\n",
    "set_session(sess3)\n",
    "model_top.load_weights(\"model_best.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_faces(image):\n",
    "    global sess\n",
    "    global graph\n",
    "    with graph.as_default():\n",
    "        set_session(sess)\n",
    "        faces = detector.detect_faces(image)\n",
    "        return np.array(faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_face_from_image(image, required_size=(IMAGE_HEIGHT, IMAGE_WIDTH)):\n",
    "    face = detect_faces(image) # content of face is a python dict\n",
    "\n",
    "    if len(face) == 0:\n",
    "        return []\n",
    "    else:\n",
    "        # extract the bounding box from the requested face\n",
    "        box = np.asarray(face[0]['box'])\n",
    "        box[box < 0] = 0\n",
    "        x1, y1, width, height =  box\n",
    "\n",
    "        x2, y2 = x1 + width, y1 + height\n",
    "        # extract the face\n",
    "        face_boundary = image[y1:y2, x1:x2]\n",
    "\n",
    "        # resize pixels to the model size\n",
    "        face_image = Image.fromarray(face_boundary)\n",
    "        face_image = face_image.resize(required_size)\n",
    "        face_array = asarray(face_image)\n",
    "        return face_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def get_face_embedding(face):\n",
    "#    samples = asarray(face, 'float32')\n",
    "#    # prepare the data for the model\n",
    "#    samples = preprocess_input(samples, version=2)\n",
    "#    \n",
    "#    global sess2\n",
    "#    global graph\n",
    "#    with graph.as_default():\n",
    "#        set_session(sess2)\n",
    "#        output = model_VGGFace.predict(samples)\n",
    "#        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_prediction(path):\n",
    "        #converting image to RGB color and save it\n",
    "        img = cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        #detect face in image, crop it then resize it\n",
    "        face = extract_face_from_image(img)\n",
    "        \n",
    "        if face == []:\n",
    "            return None, None\n",
    "        else:\n",
    "            print(\"Face detected\")\n",
    "            if face.ndim == 3:\n",
    "                face = face.reshape((1, face.shape[0], face.shape[1], face.shape[2]))\n",
    "            face = np.array(face)\n",
    "            \n",
    "            clear_session()\n",
    "            global sess3\n",
    "            global graph\n",
    "            with graph.as_default():\n",
    "                set_session(sess3)\n",
    "                out1, out2 = model_top.predict(face) #make prediction and display the result\n",
    "                val = out1[0][0] * 10\n",
    "                ar = out2[0][0] * 10\n",
    "                print(\"result: \" + str(val) + \", \" + str(ar))\n",
    "                return val, ar    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ProcessStream - Tasks:\n",
    "         Task 1: Use Multithreading for GUI + Processing\n",
    "         Task 2: Get pixels as an Input Stream (using OpenCV)\n",
    "         https://theailearner.com/2018/10/16/recording-a-specific-window-using-opencv-python/\n",
    "         Task 3: Preprocessing & Landmark detection\n",
    "         Task 4: Read in Python Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessStream(QThread):   \n",
    "    output = pyqtSignal(object)\n",
    "    def __init__(self):\n",
    "        super(ProcessStream, self).__init__()\n",
    "        self.active = True\n",
    "\n",
    "    def run(self):\n",
    "        # Task 1: Use Multithreading ???   for GUI + Input Stream\n",
    "        # Task 2: Get pixels as an Input Stream (using OpenCV)\n",
    "        # Task 3: Preprocessing & Landmark detection\n",
    "        # Task 4: Read in Python Model\n",
    "        image = ImageGrab.grab()\n",
    "        height,width,channel = np.array(image).shape\n",
    "\n",
    "        out = cv2.VideoWriter('video.avi',cv2.VideoWriter_fourcc(*'DIVX'), 5, (width,height))\n",
    "        \n",
    "        val_list = []\n",
    "        arr_list = []\n",
    "        i = 0\n",
    "        \n",
    "        while self.active == True:\n",
    "            # image = ImageGrab.grab(rect)\n",
    "            image = ImageGrab.grab()\n",
    "            out.write(cv2.cvtColor(np.array(image), cv2.COLOR_BGR2RGB))\n",
    "            \n",
    "            imgUMat = np.float32(image)\n",
    "            cv2.imwrite(\"test.jpg\", imgUMat)\n",
    "\n",
    "            val, arr = return_prediction(\"test.jpg\")\n",
    "            if val != None:\n",
    "                val_list.append(val)\n",
    "                arr_list.append(arr)\n",
    "                if len(val_list) > int(ER_SECONDS / SNAP_SECONDS):\n",
    "                    val_list.pop(0)\n",
    "                    arr_list.pop(0)\n",
    "                \n",
    "                i = i + 1\n",
    "                if i % IM == 0:\n",
    "                    valence = sum(val_list) / len(val_list)\n",
    "                    arousal = sum(arr_list) / len(arr_list)\n",
    "                    now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                    result = str(now) + \"   Valence: \" + str(round(valence, 4)) + \"\\n\" + str(now) + \"   Arousal: \" + str(round(arousal, 4))\n",
    "                    self.output.emit(result)\n",
    "                \n",
    "            time.sleep(int(INTERVAL_SECONDS/IMAGES_PER_INTERVAL))\n",
    "        out.release()\n",
    "        \n",
    "    def stop(self):\n",
    "        print(\"STOP Thread\")\n",
    "        self.active = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class CreateContent(threading.Thread):\n",
    "#    def dynamicContentCreation():\n",
    "        # Last step in Prototype\n",
    "        # Speech Recognition ( Speech -> to -> Text )\n",
    "        # Make us of a 'faked' chat-bot (decision tree)\n",
    "#        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytics got started!\n",
      "WARNING:tensorflow:From c:\\users\\tobias\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\tobias\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face detected\n",
      "result: 0.5875082686543465, 1.9619350135326385\n",
      "Face detected\n",
      "result: -0.20952671766281128, 1.7156150937080383\n",
      "Face detected\n",
      "result: -0.10765597224235535, 1.688503473997116\n",
      "Face detected\n",
      "result: -0.1770811714231968, 2.027571201324463\n",
      "Face detected\n",
      "result: -0.10601568967103958, 2.239316701889038\n",
      "Face detected\n",
      "result: -1.6132524609565735, 3.465612232685089\n",
      "Face detected\n",
      "result: 0.11138797737658024, 1.4457005262374878\n",
      "Face detected\n",
      "result: 0.031198605429381132, 1.6897067427635193\n",
      "Face detected\n",
      "result: 0.22774465382099152, 2.2624383866786957\n",
      "Face detected\n",
      "result: -0.23171847686171532, 1.9432294368743896\n",
      "Face detected\n",
      "result: -0.04243884701281786, 1.7992478609085083\n",
      "Face detected\n",
      "result: -0.1907404512166977, 2.5669029355049133\n",
      "Face detected\n",
      "result: 0.27493100613355637, 1.3767020404338837\n",
      "Face detected\n",
      "result: -0.17132019624114037, 1.7614257335662842\n",
      "Face detected\n",
      "result: 0.03389077028259635, 1.9700779020786285\n",
      "Face detected\n",
      "result: -0.09554079733788967, 2.0438985526561737\n",
      "Face detected\n",
      "result: -0.13645457103848457, 1.9880466163158417\n",
      "Face detected\n",
      "result: 0.04514290485531092, 2.067800611257553\n"
     ]
    }
   ],
   "source": [
    "# Create GUI with PyQt\n",
    "selected_window = \"\"\n",
    "\n",
    "class Ui_MainWindow(object): \n",
    "    def setupUi(self, window): \n",
    "        super().__init__()\n",
    "        self.window = window\n",
    "        #self.window.box_selection.addItems(inputList)\n",
    "        self.window.btn_start.clicked.connect(self.getSelection)\n",
    "        self.recording = False\n",
    "        self.totalResults = \"\"\n",
    "        #self.thread1 = ProcessStream(self.window.box_selection, window)\n",
    "        self.thread1 = ProcessStream()\n",
    "        self.thread1.output.connect(self.addResults)\n",
    "        app.aboutToQuit.connect(self.closeEvent)\n",
    "        \n",
    "    def getSelection(self): \n",
    "        if self.recording == False:\n",
    "            #selected_window = str(self.window.box_selection.currentText())\n",
    "            #print(\"Analytics got started! Selected window: \" + selected_window)\n",
    "            print(\"Analytics got started!\")\n",
    "            self.recording = True\n",
    "            # changing the text of label after button got clicked \n",
    "            self.window.btn_start.setText(\"Stop Analytics\")\n",
    "            self.thread1.start() # This actually causes the thread to run\n",
    "        else:\n",
    "            self.recording = False\n",
    "            self.thread1.stop()\n",
    "                        \n",
    "            self.window.btn_start.setText(\"Start Recording\")\n",
    "            # self.thread1 = ProcessStream(self.window.box_selection, self.window)  # recreate thread\n",
    "            self.thread1 = ProcessStream()\n",
    "            self.thread1.output.connect(self.addResults)\n",
    "    \n",
    "    def closeEvent(self):\n",
    "        print('Close button pressed')\n",
    "        self.recording = False\n",
    "        self.thread1.stop()\n",
    "        \n",
    "        if IPYTHON:\n",
    "            app.deleteLater\n",
    "        else:\n",
    "            sys.exit(0)\n",
    "    \n",
    "    def addResults(self, inputText):\n",
    "        self.totalResults = (inputText + \"\\n\" + self.totalResults)\n",
    "        self.window.box_results.setText(self.totalResults)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":  \n",
    "    app = QtWidgets.QApplication(sys.argv)  \n",
    "    window = uic.loadUi(\"dialog_2.ui\")\n",
    "    \n",
    "    myApp = Ui_MainWindow()  \n",
    "    # myApp.setupUi(window, getWindowNames())\n",
    "    myApp.setupUi(window)  \n",
    "      \n",
    "      \n",
    "    if IPYTHON == False:\n",
    "        window.show() \n",
    "        sys.exit(app.exec_())\n",
    "    else:\n",
    "        window.show()\n",
    "        app.exec_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
